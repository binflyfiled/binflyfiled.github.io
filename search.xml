<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>美好事物</title>
      <link href="/2021/1201undefined.html"/>
      <url>/2021/1201undefined.html</url>
      
        <content type="html"><![CDATA[<p><img src="/2021/1201undefined/1.jpg" alt="1"><br><img src="/2021/1201undefined/2.jpg" alt="2"><br><img src="/2021/1201undefined/3.jpg" alt="3"> </p>]]></content>
      
      
      
        <tags>
            
            <tag> 20安五撕名牌大战 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>青年大学习表格注意事项</title>
      <link href="/2021/120122818.html"/>
      <url>/2021/120122818.html</url>
      
        <content type="html"><![CDATA[<pre><code>1.21级统一拉出来，置于新生院下2.格式:        字体宋体11号，行距24.95磅，列宽14.9；        背景颜色填充无，上下左右居中对齐        固定表头3.变化：    超百的不再归百，团员人数按网站来    低于70%标红，不要标多4.命名2021年秋季学习"青年大学习"第X季第X期主题团课学习情况汇总。</code></pre><p>##数据来源和数据解析#<br><img src="/2021/120122818/%E6%95%B0%E6%8D%AE%E6%9D%A5%E6%BA%90.png" alt="数据来源和数据解析截图"><br><img src="/2021/120122818/%E6%95%B0%E6%8D%AE%E8%A7%A3%E6%9E%90.jpg" alt="数据来源和数据解析截图"></p><h2 id="标红问题"><a href="#标红问题" class="headerlink" title="标红问题"></a>标红问题</h2><p><img src="/2021/120122818/%E6%A0%87%E7%BA%A2%E9%97%AE%E9%A2%98.jpg" alt="标红问题"></p><h2 id="格式问题"><a href="#格式问题" class="headerlink" title="格式问题#"></a>格式问题#</h2><p><img src="/2021/120122818/%E6%A0%BC%E5%BC%8F%E9%97%AE%E9%A2%98.jpg" alt="格式问题"></p><h2 id="计算公式注意"><a href="#计算公式注意" class="headerlink" title="计算公式注意#"></a>计算公式注意#</h2><p><img src="/2021/120122818/%E8%AE%A1%E7%AE%97%E5%85%AC%E5%BC%8F%E6%98%AF%E5%B9%B3%E5%9D%87.jpg" alt="计算公式是平均"><br>##列宽和行高#<br><img src="/2021/120122818/%E8%A1%8C%E9%AB%98%E8%AE%BE%E7%BD%AE%E4%B8%BA24.95.jpg" alt="行高设置为24.95"><br>##自定义排序#<br><img src="/2021/120122818/%E8%87%AA%E5%AE%9A%E4%B9%89%E6%8E%92%E5%BA%8F.jpg" alt="自定义排序"></p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>七堇年</title>
      <link href="/2021/112338559.html"/>
      <url>/2021/112338559.html</url>
      
        <content type="html"><![CDATA[<p>我们的年少单薄，使得我们常常因为不知道痛苦这个词语的真相而轻易亲近这个概念，将自己的脆弱装裱为痛苦，并隆重展览，希望博取他人一点驻足和关注。<br>——七堇年</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>EDG夺冠和北京大雪</title>
      <link href="/2021/111032534.html"/>
      <url>/2021/111032534.html</url>
      
        <content type="html"><![CDATA[<p>11月7日，对我来说世间就有两件事，一个是EDG夺冠了，一个是北京下雪了。他们对我的触动很大，我不想仅仅将他们浪费在记忆的长河，被遗忘冰封。</p><p>这两件事情，我无法将其排列，所以想混着来说。昨天夜里，再输一局，中国队就回家了，那个时候我想了很多，大多是自己的成长经历。不知有多少次，</p><p>面对着困难失败，没有翻越它而是选择了像往常一样呆在自己的舒适区里，以前我以为那算作能力不足却面对困难的倔强,但现在改变了看法。</p><p>DK气势汹汹下了EDG两成了，大部分朋友说:”硬实力差距，没办法”、“不看了，肯定3:1”，“不可能了，睡吧”，“第一把打爽了，拿下DK一城，EDG已经算赢了”，我觉得朋友们说的很有道理，本来就是呀，EDG失败了6年，面对的是上届冠军DK,S赛无一败北。可当时我不知道为什么，就想起了童年，童年的什么啊，不明白，就是那种集体战斗不服输的感觉，就如斯巴达300勇士们、</p><p>就如抗美援朝长津湖的冰冻连，过了一会终于想起来了——我相信光——迪迦大结局。我相信也有很多小伙伴有这种感觉，于是我告诉垂头丧气的同学，请相信光。</p><p>在很多时候，我觉得有神的存在，上天总会安排一些巧合的预兆，来教会你很多道理。</p><p>EDG抗住了DK第4把，成功扳平。那个时候北京雪越下越大，宿舍外愈来愈冷，我想起林冲风雪山神庙的雪夜，山伺银龙，越发赞叹生命的神奇和美丽，那时我觉得，上天也许就像我们一样，是有温度的、有情感的人。我也为自己半夜不睡、只为一游戏竞赛疯狂而感到羞怯，这是社会上非年轻一代的主流观点，我认为这是正确的价值观，但我作为青年，却毫无理由的选择了背弃它。背弃它，我的心，在思考的时候总是隐隐作痛。想起自己来北京的不易，闭塞的小村子里冬天也是常有鹅毛大雪，有那么几次，大雪封路，我眼里含着泪，回想着被寒风吹红的母亲的脸，想起她身上披着一层一层的棉衣，想起现在被禁止的电动三轮车和因它而生的车篷子；想起弟弟肉嘟嘟的脸对着玻璃窗外的寒天哈气，想起他可爱的身体坐在炕头上裹着棉衣；想起爸爸12月份回家时坐的绿皮火车上覆盖的白雪，想起爷爷逝世前自己孤独一人守过的寒夜里又冷又热的篝火……</p><p>EDG与DK已经进行完bp，解说担忧的说出来，我们都明白，阵容上实在是差它一等。但是那是我的心里信念却很坚，定不是为自己看到的银龙的预兆而坚定，而是已经打到了bo5，那些所谓的硬实力都随他去吧，打的认真、突破舒适区，这就是我们要做的。我技术很差，赛点到底是在哪里决定的，我说不准确，只是最后我看到了他们的突破，我想全国玩家都明白他们的一贯作风，他们改变了。随后大势已经扭转，就好像我党百万雄师渡长江后如入无人之地，没过多久，胜利了胜利了！</p><p>我停了一会，感觉世界好像睡了，风景好像睡了，故事好像睡了，我真哭了，银龙还在飞舞、江山还在浩荡、林冲还在跋涉。接下来就是爆炸，我所在的学校还好，就欢呼了一小阵，声音也不是很大，但是这宿舍楼里的人应该都没睡，虚拟网络中的势头变得不对劲，由胜利的欢呼转变成理性的审丑，这让我开始深思，资本是丑恶的，但是资本的丑恶是不是人性丑恶的外在表现呢？我觉得我学识浅薄，见识短浅，知事情之定性却无本领去拨开看本质而大化之。我们宿舍看网友的各种奇葩flag笑疯了1个小时，最后我愧疚的入睡，床上已经是一篇狼藉，虽然如此却有小苏学士赤壁之乐。</p><p>那个夜，睡觉其实挺难的，百感交集，激动无比，第二天醒来后，脑子里留下的是写一篇深思文，于是就写了个这。</p><p>我大致是8.30起床，雪软绵绵的下个不完，不进早食（应该没有了），我去校园里走了一遭，我看同学们玩雪、拍照，觉得他们很青涩，一看警号就明白，大都是21的新生，老生都在宿舍里面。</p><p>我想去忠诚广场，去看看忠诚雕像，我在公园小路口，看到一个师妹在背英语文章，似乎是罗素的文章。在路上，虽然单薄的身体懂的渐无知觉，但是心还是有的，我觉得我们这些大二的老生应当像新生一样保持活力，只有这样才能接好长辈的红旗。前面我说过，我毫无疑问的选择了背弃传统主流价值观，在某种意义上，我觉得这就是不破不立，那些东西是肯定的，是好的，但是我们仅在那个圈子里，怎能去开拓想要的成就，走出了舒适区，会有一段时间的消沉，但是只有这样才能有突破，在新时代下，这是我们中华民族大部分同胞所缺少的一种本性，愿这种不破不立能够成为中国当下年轻人的标志。</p>]]></content>
      
      
      
        <tags>
            
            <tag> 逐渐认识自我 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hello World</title>
      <link href="/2021/1110undefined.html"/>
      <url>/2021/1110undefined.html</url>
      
        <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>事件脉络还原</title>
      <link href="/2021/110836867.html"/>
      <url>/2021/110836867.html</url>
      
        <content type="html"><![CDATA[<h2 id="依赖的第三方库"><a href="#依赖的第三方库" class="headerlink" title="依赖的第三方库"></a>依赖的第三方库</h2><pre><code>import gensim.models##模型封装库import pandas as pdimport numpy as np##数据处理的常用神库from pyecharts.charts import HeatMapfrom pyecharts import options as optsfrom pyecharts.render import make_snapshotfrom snapshot_selenium import snapshot##与echarts接口的pyecharts，用来可视化结果</code></pre><h2 id="所用语料库"><a href="#所用语料库" class="headerlink" title="所用语料库"></a>所用语料库</h2><p>大名鼎鼎的Tencent_ChineseEmbedding——为超过800万个汉语词汇提供了200维向量表征。</p><hr><p>下载链接<a href="ai.tencent.com/ailab/nlp/embedding.html">ai.tencent.com/ailab/nlp/embedding.html</a></p><h2 id="模型思路"><a href="#模型思路" class="headerlink" title="模型思路"></a>模型思路</h2><p>“代填写”</p><hr><h2 id="所利用模型解读"><a href="#所利用模型解读" class="headerlink" title="所利用模型解读"></a>所利用模型解读</h2><h3 id="LCS"><a href="#LCS" class="headerlink" title="LCS###"></a>LCS###</h3><pre><code>def LCS(s1, s2):#为了处理长词汇问题以及一些简单的词匹配#生成字符串长度加1的0矩阵，m用来保存对应位置匹配的结果    m = [[0 for x in range(len(s2) + 1)] for y in range(len(s1) + 1)]    #d用来记录转移方向    d = [[None for x in range(len(s2) + 1)] for y in range(len(s1) + 1)]    for p1 in range(len(s1)):        for p2 in range(len(s2)):            if s1[p1] == s2[p2]:  #符匹配成功，则该位置的值为左上方的值加1                m[p1 + 1][p2 + 1] = m[p1][p2] + 1                d[p1 + 1][p2 + 1] = 'ok'            elif m[p1 + 1][p2] &gt; m[p1][p2 + 1]:  #左值大于上值，则该位置的值为左值，并标记回溯时的方向                m[p1 + 1][p2 + 1] = m[p1 + 1][p2]                d[p1 + 1][p2 + 1] = 'left'            else:  #上值大于左值，则该位置的值为上值，并标记方向up                m[p1 + 1][p2 + 1] = m[p1][p2 + 1]                d[p1 + 1][p2 + 1] = 'up'    (p1, p2) = (len(s1), len(s2))    # print(np.array(d))    s = []    while m[p1][p2]:  #不为None时        c = d[p1][p2]        if c == 'ok':  #匹配成功，插入该字符，并向左上角找下一个            s.append(s1[p1 - 1])            p1 -= 1            p2 -= 1        if c == 'left':  #根据标记，向左找下一个            p2 -= 1        if c == 'up':  #根据标记，向上找下一个            p1 -= 1    s.reverse()    return ''.join(s)#</code></pre><hr><p>LCS又称最大公共子序列方法，实现结果是返回两个文本句子中含有的最长的公共子序列。公共子序列不一定要求连续，但需要保持位置出现的先后关系。例如：apim:”小王打了小程抢了他的钱并逃回了十号楼”，bpim:”小段打了小程抢了他的钱并逃回了九号楼”。最后可以呈现的公共子序列是”小打了小程抢了他的钱并逃回了号楼”</p><hr><p>###similarity###</p><pre><code>def similarity(apim,bpim):    # 利用大型文本语料库，计算两个词之间的余弦相似度    # 获取两个词之间的余弦相似度        apim =str(apim)        bpim = str(bpim)        if apim == "无":            if bpim == "无":                return 1            else:                return 0        elif bpim == "无":            return 0    try:        apim = str(apim)        bpim = str(bpim)        result = model.wv.similarity(apim, bpim)    except:        len_apim = len(apim)        len_result = len(LCS(apim,bpim))        if len_result/len_apim&gt;= 0.5:            result = len_result/len_apim        else:            result = 0    return result</code></pre><p>这个函数是对Word2Vec模型的基本应用，拥有越健全的语料库训练出来的模型往往越接近人的常识，所以我们选择<br>“Tencent_ChineseEmbedding”。<br>对于不存在的词，退而求其次，应用LCS待而处理。</p><hr><p>###proof_cross###</p><pre><code>def proof_cross(dot_A,dot_B,w):""":param dot_A: 1*10[]  这是主文本的一个阶段:param dot_B: n*10[]  这是客文本的所有阶段:param w: 1*10[] 权重AA:return:"""    result_dot = []    for i in range(len(dot_B)):        dot_C = list(map(lambda x,y:similarity(x,y),dot_A,dot_B[i]))        dot_D = list(map(lambda x,y:x*y,dot_C,w))        result = sum(dot_D)        result_dot.append(result)    max_sentence = max(result_dot)    max_sentence_index = result_dot.index(max_sentence)    return max_sentence,max_sentence_index  </code></pre><p>模型句子对齐靠这个函数实现，他的原理就是不再依赖简单的for循环，利用准备好的主文本阶段矩阵和客文本全阶段矩阵,类似于矩阵叉乘,寻找出客文本中与该主文本阶段最相近的阶段，返回相似度和阶段索引。这样顺利完成了工作任务而且简化了复杂的，但是还需要一个验证函数。</p><hr><p>###verification和n_LCS###</p><pre><code>def verification(listA,listB,index = [0,1,8,3,4,6,7,9]):    Merged_sentences_A = ""    Merged_sentences_B = ""    for i in index:        if listA[i] != "无":            Merged_sentences_A+=str(listA[i])    for i in index:        if listB[i] != "无":            Merged_sentences_B+=str(listB[i])    result = n_LCS(Merged_sentences_A,Merged_sentences_B)    return resultdef n_LCS(s1,s2):#计算LCS后，主文本与客文本的相似度，用主文本与最大公共子序列作为计算筹码    new_sentence = LCS(s1,s2)    if len(new_sentence)/len(s1)&lt;1:        return len(new_sentence)/len(s1)    else:        return 1</code></pre><p>proof_cross返回了最相似度阶段，它是否值得我们信赖？我们用verification()检验就能得出结论。检验的方法是对句子应用n_LCS函数，n_LCS是以主句子与公共子序列的长度大小来计算相似度的，因为拥有越多的相似词的子序列与主句子的相似度越高，与词向量匹配模型在某些方面存在互补性，可以用来检验similarity是否合理。</p><hr><pre><code>path = r"C:\Users\BIN\Desktop\data\新数据\大聪明合成\凤雏.xlsx"len_sheet = 3#一个案件的不同人说出的证据next_list = []#将证据文本转化到程序中for j in range(1,len_sheet+1):    df_tem = pd.read_excel(path,dtype=np.object,sheet_name=f"Sheet{j}")    df_tem.columns = [i for i in range(len(df_tem.columns))]    df_tem.set_index(0,inplace=True)    df_tem.fillna("无",inplace = True)    tem_list = df_tem.T.values.tolist()    next_list.append(tem_list)</code></pre><p>将标签文本导入程序</p><hr><p>###create_proof_nature###</p><pre><code>def create_proof_nature(form_shape,number):"""定义更高一维的对齐矩阵:param form_shape::param number::return:"""    proof_sen_value = {}    for i in range(number):        proof_sen_value[i] = form_shape    return proof_sen_valueproof_sen_value = create_proof_nature(pd.DataFrame(),3)proof_sen = proof_sen_value.copy()proof_story = proof_sen_value.copy()</code></pre><p>创建对齐矩阵</p><hr><p>###主要运行步骤###</p><pre><code>#定义权重w = [0.09,0.09,0.09,0.19,0.09,0.09,0.09,0.09,0.09,0.09]#核心处理df = pd.DataFrame()columns = ["时间","地点","附属主体","主体","行为","附属客体","客体","补语","造成原因", "结果","相似度","sheet1","sheet2","sheet3"]host_k = 0homeandguest = []for proof in next_list:    proof_sen_value[host_k] = pd.DataFrame(np.zeros((len(proof),len_sheet)))    proof_sen[host_k] = pd.DataFrame(np.zeros((len(proof), len_sheet)))    df = pd.DataFrame()    # 每个证据    sentence_k = -1    for sentence in proof:        sentence_k+=1        # 主文本的某个句子        for guest_k in range(len(next_list)):            # 对比文本全部            max_sentence, max_sentence_index = proof_cross(sentence, next_list[guest_k], w)#proof_cross利用将返回一个句子对应一个文本里的最有可能相似的阶段的相似度和索引            a = next_list[host_k][sentence_k].copy()            a.append(max_sentence)            if verification(next_list[host_k][sentence_k],next_list[guest_k][max_sentence_index])&gt;0.5:#verification 检验要匹配的阶段是否够资格，如果够资格，将输入到对齐矩阵中                print("yes!")                proof_sen_value[host_k].at[sentence_k,guest_k] =max_sentence#修改为匹配度                proof_sen[host_k].at[sentence_k, guest_k]  = max_sentence_index#修改为阶段索引                b = next_list[guest_k][max_sentence_index].copy()                a.append([b])            else:                proof_sen_value[host_k].at[sentence_k,guest_k] = max_sentence#修改为匹配度                proof_sen[host_k].at[sentence_k, guest_k] = None                a.append(0)            df = df.append(pd.DataFrame([a]))    proof_story[host_k] = df    host_k+=1</code></pre><hr><p>###热力图Heap###</p><pre><code>for i in range(len(proof_sen_value)):    data=[]    for j in range(len(proof_sen_value[i])):        for k in range(len(proof_sen_value[i].iloc[j,:])):            data.append([j,k,round(proof_sen_value[i].iloc[j,k]*100,1)])    # data = [[j,k,proof_sen_value[i][j][k]] for k in range(len(proof_sen_value[i][j])) for j in range(len(proof_sen_value[i])) ]    xaxis = [ l  for l in range(len(proof_sen_value[i])) ]    yaxis = [f"sheet{l}" for l in proof_sen_value[i]]    heat = (HeatMap()            .add_xaxis(xaxis)            .add_yaxis("语句编号",yaxis,                       data,                       label_opts=opts.LabelOpts(is_show=True, position="inside"))            .set_global_opts(                title_opts=opts.TitleOpts(title="HeatMap-可信度", subtitle="proof{}".format(i)),                visualmap_opts=opts.VisualMapOpts(),                legend_opts=opts.LegendOpts(is_show=False))           )    make_snapshot(snapshot,heat.render(),f"D:\\Data\\凤雏\\凤雏可视化\\test{i}.png")</code></pre><p>通过pyecharts库生成热力图</p><hr><p>###三种 对齐矩阵输出到xlsx文件中###</p><pre><code>#proof_sen.xlsx 匹配到的标签索引#proof_sen_value 所有的匹配值#proof_story 主文本阶段与匹配的阶段联合起来的展示表writer = pd.ExcelWriter(r"D:\Data\凤雏\proof_sen.xlsx")for i in range(len(proof_sen)):    proof_sen[i].to_excel(excel_writer = writer,sheet_name = f"Sheet{i}")writer.save()writer.close()writer = pd.ExcelWriter(r"D:\Data\凤雏\proof_sen_value.xlsx")for i in range(len(proof_sen_value)):    proof_sen_value[i].to_excel(excel_writer = writer,sheet_name = f"Sheet{i}")writer.save()writer.close()writer = pd.ExcelWriter(r"D:\Data\凤雏\proof_story.xlsx")for i in range(len(proof_story)):    proof_story[i].to_excel(excel_writer = writer,sheet_name = f"Sheet{i}")writer.save()writer.close()</code></pre><hr><p>##所有代码##</p><pre><code>import osimport gensim.modelsimport pandas as pdimport numpy as npfrom pyecharts.charts import HeatMapfrom pyecharts import options as optsfrom pyecharts.render import make_snapshotfrom snapshot_selenium import snapshotdf = pd.DataFrame()#######################################################函数和模型准备# model_file = r'C:\Users\BIN\Desktop\NLP\nlp_bd.model'    # 加载模型file = r"D:\Data\Tencent_ChineseEmbedding.bin"model = gensim.models.KeyedVectors.load(file, mmap='r')def LCS(s1, s2):    #为了处理长词汇问题以及一些简单的词匹配    #生成字符串长度加1的0矩阵，m用来保存对应位置匹配的结果    m = [[0 for x in range(len(s2) + 1)] for y in range(len(s1) + 1)]    #d用来记录转移方向    d = [[None for x in range(len(s2) + 1)] for y in range(len(s1) + 1)]    for p1 in range(len(s1)):        for p2 in range(len(s2)):            if s1[p1] == s2[p2]:  #符匹配成功，则该位置的值为左上方的值加1                m[p1 + 1][p2 + 1] = m[p1][p2] + 1                d[p1 + 1][p2 + 1] = 'ok'            elif m[p1 + 1][p2] &gt; m[p1][p2 + 1]:  #左值大于上值，则该位置的值为左值，并标记回溯时的方向                m[p1 + 1][p2 + 1] = m[p1 + 1][p2]                d[p1 + 1][p2 + 1] = 'left'            else:  #上值大于左值，则该位置的值为上值，并标记方向up                m[p1 + 1][p2 + 1] = m[p1][p2 + 1]                d[p1 + 1][p2 + 1] = 'up'    (p1, p2) = (len(s1), len(s2))    # print(np.array(d))    s = []    while m[p1][p2]:  #不为None时        c = d[p1][p2]        if c == 'ok':  #匹配成功，插入该字符，并向左上角找下一个            s.append(s1[p1 - 1])            p1 -= 1            p2 -= 1        if c == 'left':  #根据标记，向左找下一个            p2 -= 1        if c == 'up':  #根据标记，向上找下一个            p1 -= 1    s.reverse()    return ''.join(s)#def n_LCS(s1,s2):    #计算LCS后，主文本与客文本的相似度，用主文本与最大公共子序列作为计算筹码    new_sentence = LCS(s1,s2)    if len(new_sentence)/len(s1)&lt;1:        return len(new_sentence)/len(s1)    else:        return 1def similarity(apim,bpim):    # 利用大型文本语料库，计算两个词之间的余弦相似度    # 获取两个词之间的余弦相似度    apim =str(apim)    bpim = str(bpim)    if apim == "无":        if bpim == "无":            return 1        else:            return 0    elif bpim == "无":        return 0    try:        apim = str(apim)        bpim = str(bpim)        result = model.wv.similarity(apim, bpim)    except:        len_apim = len(apim)        len_result = len(LCS(apim,bpim))        if len_result/len_apim&gt;= 0.5:            result = len_result/len_apim        else:            result = 0    return resultdef proof_cross(dot_A,dot_B,w):    """    :param dot_A: 1*10[]  这是主文本的一个阶段    :param dot_B: n*10[]  这是客文本的所有阶段    :param w: 1*10[] 权重AA    :return:    """    result_dot = []    for i in range(len(dot_B)):        dot_C = list(map(lambda x,y:similarity(x,y),dot_A,dot_B[i]))        dot_D = list(map(lambda x,y:x*y,dot_C,w))        result = sum(dot_D)        result_dot.append(result)    max_sentence = max(result_dot)    max_sentence_index = result_dot.index(max_sentence)    return max_sentence,max_sentence_indexdef verification(listA,listB,index = [0,1,8,3,4,6,7,9]):    Merged_sentences_A = ""    Merged_sentences_B = ""    for i in index:        if listA[i] != "无":            Merged_sentences_A+=str(listA[i])    for i in index:        if listB[i] != "无":            Merged_sentences_B+=str(listB[i])    result = n_LCS(Merged_sentences_A,Merged_sentences_B)    return resultdef create_proof_nature(form_shape,number):    """    定义更高一维的对齐矩阵    :param form_shape:    :param number:    :return:    """    proof_sen_value = {}    for i in range(number):        proof_sen_value[i] = form_shape    return proof_sen_valuepath = r"C:\Users\BIN\Desktop\data\新数据\大聪明合成\凤雏.xlsx"len_sheet = 3#一个案件的不同人说出的证据next_list = []#将证据文本转化到程序中for j in range(1,len_sheet+1):    df_tem = pd.read_excel(path,dtype=np.object,sheet_name=f"Sheet{j}")    df_tem.columns = [i for i in range(len(df_tem.columns))]    df_tem.set_index(0,inplace=True)    df_tem.fillna("无",inplace = True)    tem_list = df_tem.T.values.tolist()    next_list.append(tem_list)#创建对齐矩阵proof_sen_value = create_proof_nature(pd.DataFrame(),3)proof_sen = proof_sen_value.copy()proof_story = proof_sen_value.copy()#定义权重w = [0.09,0.09,0.09,0.19,0.09,0.09,0.09,0.09,0.09,0.09]#核心处理df = pd.DataFrame()columns = ["时间","地点","附属主体","主体","行为","附属客体","客体","补语","造成原因", "结果","相似度","sheet1","sheet2","sheet3"]host_k = 0homeandguest = []for proof in next_list:    proof_sen_value[host_k] = pd.DataFrame(np.zeros((len(proof),len_sheet)))    proof_sen[host_k] = pd.DataFrame(np.zeros((len(proof), len_sheet)))    df = pd.DataFrame()    # 每个证据    sentence_k = -1    for sentence in proof:        sentence_k+=1        # 主文本的某个句子        for guest_k in range(len(next_list)):            # 对比文本全部            max_sentence, max_sentence_index = proof_cross(sentence, next_list[guest_k], w)#proof_cross利用将返回一个句子对应一个文本里的最有可能相似的阶段的相似度和索引            a = next_list[host_k][sentence_k].copy()            a.append(max_sentence)            if verification(next_list[host_k][sentence_k],next_list[guest_k][max_sentence_index])&gt;0.5:#verification 检验要匹配的阶段是否够资格，如果够资格，将输入到对齐矩阵中                print("yes!")                proof_sen_value[host_k].at[sentence_k,guest_k] =max_sentence#修改为匹配度                proof_sen[host_k].at[sentence_k, guest_k]  = max_sentence_index#对应位置修改为当前阶段索引                b = next_list[guest_k][max_sentence_index].copy()                a.append([b])            else:                proof_sen_value[host_k].at[sentence_k,guest_k] = max_sentence                proof_sen[host_k].at[sentence_k, guest_k] = None                a.append(0)            df = df.append(pd.DataFrame([a]))    proof_story[host_k] = df    host_k+=1# 生成热力图# data = [[i, j, random.randint(0, 50)] for i in range(24) for j in range(7)]for i in range(len(proof_sen_value)):    data=[]    for j in range(len(proof_sen_value[i])):        for k in range(len(proof_sen_value[i].iloc[j,:])):            data.append([j,k,round(proof_sen_value[i].iloc[j,k]*100,1)])    # data = [[j,k,proof_sen_value[i][j][k]] for k in range(len(proof_sen_value[i][j])) for j in range(len(proof_sen_value[i])) ]    xaxis = [ l  for l in range(len(proof_sen_value[i])) ]    yaxis = [f"sheet{l}" for l in proof_sen_value[i]]    heat = (HeatMap()            .add_xaxis(xaxis)            .add_yaxis("语句编号",yaxis,                       data,                       label_opts=opts.LabelOpts(is_show=True, position="inside"))            .set_global_opts(                title_opts=opts.TitleOpts(title="HeatMap-可信度", subtitle="proof{}".format(i)),                visualmap_opts=opts.VisualMapOpts(),                legend_opts=opts.LegendOpts(is_show=False))           )    make_snapshot(snapshot,heat.render(),f"D:\\Data\\凤雏\\凤雏可视化\\test{i}.png")#三种 对齐矩阵输出到xlsx文件中#proof_sen.xlsx 匹配到的标签索引#proof_sen_value 所有的匹配值#proof_story 主文本阶段与匹配的阶段联合起来的展示表writer = pd.ExcelWriter(r"D:\Data\凤雏\proof_sen.xlsx")for i in range(len(proof_sen)):    proof_sen[i].to_excel(excel_writer = writer,sheet_name = f"Sheet{i}")writer.save()writer.close()writer = pd.ExcelWriter(r"D:\Data\凤雏\proof_sen_value.xlsx")for i in range(len(proof_sen_value)):    proof_sen_value[i].to_excel(excel_writer = writer,sheet_name = f"Sheet{i}")writer.save()writer.close()writer = pd.ExcelWriter(r"D:\Data\凤雏\proof_story.xlsx")for i in range(len(proof_story)):    proof_story[i].to_excel(excel_writer = writer,sheet_name = f"Sheet{i}")writer.save()wr</code></pre>]]></content>
      
      
      
        <tags>
            
            <tag> 大二上的难离难舍 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
